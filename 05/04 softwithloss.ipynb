{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import *\n",
    "\n",
    "class SoftmaxWithLoss(object):\n",
    "    # 需要调用 激活函数softmax() 和 损失函数 cross_entropy_error()\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        # 传入的x是Affinx的forward函数的返回值，也就是[输入数据 * W + b]\n",
    "        # self.y 和 self.t 在 forward 中传入后保存，然后在backword中被使用\n",
    "        self.y = softmax(x) # 激活函数处理\n",
    "        self.t = t\n",
    "\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self,dout):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        if self.y.size == self.t.size:\n",
    "            # 如果是 one-hot 的形式\n",
    "            dx = (self.y - self.t)/batch_size # 这里不懂为啥除以 batch_size，虽然梯度时原来的1/batch_size倍，但是梯度的方向是不变的\n",
    "        else:\n",
    "            # 非 one-hot 的形式\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size),self.t] -= 1\n",
    "            dx /= batch_size\n",
    "\n",
    "        return dx\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52078a61cf360e1cb9de801769e3cf77056a7e6fc9ad1b4a4e5b78b8e2b4c3dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
