{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之前已经实现的函数，接下来要进行调用\n",
    "\n",
    "# 激活函数\n",
    "def softmax(x):\n",
    "    \"\"\"输入一维数组或者二维数组，输入一维返回一维，输入二维返回二维\"\"\"\n",
    "    if x.ndim == 2: # x=[[1,2,3],[2,3,4]]\n",
    "        x = x.T # x=[[1,2],[2,3],[3,4]]\n",
    "        x = x - np.max(x,axis=0) # x=x - [3,4] = [[1-3,2-4],[2-3,3-4],[3-3,4-4]] = [[-2,-2],[-1,-1],[0,0]]\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T # 返回二维数组\n",
    "    # x.ndim == 1 的情况\n",
    "    x = x-np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x)) # 返回一维数组比如,ret.shape=(3,)\n",
    "\n",
    "\n",
    "# 损失函数\n",
    "def cross_entropy_error(y,t):\n",
    "    \n",
    "    \"\"\"\n",
    "    可以传入一维(单个数据)或者二维(多个数据)的 y (预测数据)，一维的会在函数内转换为二维\n",
    "    传入的 t(监督数据) 可以是one-hot的形式，比如 t=[[0,0,1],[0,1,0]]，或者 t=[2,1]\n",
    "    \"\"\"\n",
    "\n",
    "    delta = 1e-7\n",
    "\n",
    "    if y.ndim == 1:  # y = [1,2,4] t = [0,0,1]\n",
    "        # 如果 ndim 为 1，为了统一，转换为 2 维\n",
    "        y = y.reshape(1,y.size)  # [[1,2,4]]\n",
    "        t = t.reshape(1,t.size)  # [[0,0,1]]\n",
    "\n",
    "    if t.size == y.size:  # 将 one-hot 形式转换为普通形式 比如 t=[[0,0,1],[0,1,0]] -->  t=[2,1]\n",
    "        t = t.argmax(axis=1)\n",
    "    \n",
    "    batch_size = y.shape[0] # 数据个数 \n",
    "    # return -np.sum(t*np.log(y + delta)) / batch_size\n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t])) / batch_size\n",
    "\n",
    "\n",
    "# 梯度\n",
    "# 下面两个函数定义了多个数据的梯度，其中每个数据可以是一元或者多元函数\n",
    "def _numerical_gradient_1d(f,x):\n",
    "    \"\"\"\n",
    "    求一元或者多元函数梯度，传入的 x 是一维数组，代表坐标，浮点数。比如 x = [1.0,2.0] 就是二元函数在 (1,2) 上的点。求的是在这个点上的二元函数的两个方向的偏导\n",
    "    \"\"\"\n",
    "    h = 1e-4\n",
    "\n",
    "    grad = np.zeros_like(x) # 假如是二元函数，传入变量 x = [3,4]，则现在 grad = [0,0]，grad[0],grad[1] 分别是二元函数的两个变量的梯度\n",
    "\n",
    "    for idx in range(x.size): # x: [3,4], idx: [0,1]\n",
    "        tmp_val = x[idx] # tmp_val=x[0]=3\n",
    "        x[idx] = tmp_val + h # x: [3+h,4]\n",
    "        fxh1 = f(x) # [3+h,4] 对应的函数值 f\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1-fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 还原x\n",
    "        \n",
    "    return grad\n",
    "    \n",
    "\n",
    "def numerical_gradient_2d(f,X):\n",
    "    \"\"\"2d数组的梯度\"\"\"\n",
    "\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_1d(f,X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X) # X=[[2,3,4],[1,2,1]], grad=[[0,0,0],[0,0,0]]\n",
    "        \n",
    "        for idx, x in enumerate(X): #  x=[2,3,4],[1,2,1], idx=0,1\n",
    "            grad[idx] = _numerical_gradient_1d(f,x)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "\n",
    "# 梯度下降函数\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=300):\n",
    "    x = init_x # 假设是二元函数，x=[2,2], f=x[0]**2+x[1]**2\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "        \n",
    "        grad = numerical_gradient_2d(f,x)  # grad=[4,4]\n",
    "        x -= lr * grad  # x = [2,2] - 0.01*[4,4] = [1.96,1.96]\n",
    "\n",
    "    return x, np.array(x_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.94265553,  0.4580235 , -1.60663573],\n",
       "       [-1.34852288,  1.01747501,  0.37895319]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 1. 定义 x t w\n",
    "x = np.array([0.6, 0.9]) # 定义的 x\n",
    "t = np.array([0,0,1]) # 正确解标签\n",
    "W = np.random.randn(2,3) # 权重\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.64807728,  1.19054161, -0.62292357])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. x,w 点积求 y\n",
    "z = np.dot(x,W)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12028887, 0.75635816, 0.12335296])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 用激活函数处理 y\n",
    "y = softmax(z)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.092705434172959"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 求损失函数\n",
    "loss = cross_entropy_error(y,t)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数loss 随着 W 的改变而改变  \n",
    "f = lambda W: cross_entropy_error(y_sm,t)  ^1   \n",
    "等价于  \n",
    "def f(W):  \n",
    "----return cross_entropy_error(y_sm,t)\n",
    "\n",
    "那么求损失函数关于 W 的梯度，那么能找到一个 W，可以使得损失函数最小，这个 W 就是训练出来的合适的 权重系数  \n",
    "dW = numerical_gradient_2d(f,W)  \n",
    "\n",
    "但是 ^1 式无法将 W 传入到 f(也就是损失函数)中，因为 W 改变，z，y都没改变，损失函数也就没改变 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet():\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t) \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21412802 -0.09228939  0.62688203]\n",
      " [ 1.6645847   1.32434296 -0.73495762]]\n"
     ]
    }
   ],
   "source": [
    "# 1. 定义 x, t, w\n",
    "net = simpleNet()\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原先的第2,3,4步包含在 simpleNet 中\n",
    "\n",
    "# 5. 把 W 和 损失函数关联\n",
    "f = lambda w: net.loss(x,t)\n",
    "\n",
    "# 等价于下面的 f\n",
    "def f(w):\n",
    "    return net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.02021979, -0.21694878,  0.94544965],\n",
       "        [ 1.37372236,  1.13735387, -0.25710619]]),\n",
       " array([[[ 0.21412802, -0.09228939,  0.62688203],\n",
       "         [ 1.6645847 ,  1.32434296, -0.73495762]],\n",
       " \n",
       "        [[ 0.14596087, -0.13404754,  0.73680733],\n",
       "         [ 1.56233398,  1.26170573, -0.57006967]],\n",
       " \n",
       "        [[ 0.08124007, -0.17578973,  0.84327032],\n",
       "         [ 1.46525277,  1.19909245, -0.41037518]]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. 总结 - 求 net.W\n",
    "gradient_descent(f, net.W, lr=0.2, step_num=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52078a61cf360e1cb9de801769e3cf77056a7e6fc9ad1b4a4e5b78b8e2b4c3dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
